---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
library(lme4)
library(ggplot2)
library(MuMIn)
library(caret)
library(pROC)
library(magrittr)


#load the data
data_all = read.csv("complete_data.csv")
data_all=na.omit(data_all)
data_all$Subject = as.factor(as.numeric(as.factor(data_all$Subject)))
data_all$ID = as.factor(as.numeric(as.factor(data_all$ID)))
#set baseline
data_all$Diagnosis = relevel(data_all$Diagnosis,ref="Control")

sum(data_all$Diagnosis=="Control")
sum(data_all$Diagnosis=="Schizophrenia")

#up sample the data
data_ups_NAs = upSample(x = data_all, y = data_all$Diagnosis, yname = "Diagnosis") 
```


## Assignment 3 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of quantitative descriptors of voice in schizophrenia, focusing on pitch.
In the course of this assignment we will use them to try to automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.

### Question 1: Can you diagnose schizophrenia from pitch range only? If so, how well?

Build a logistic regression to see whether you can diagnose schizophrenia from pitch range only.

```{r build range model}
m_range = glmer(Diagnosis ~ range + (1+Trial| Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))


summary(m_range) #significant
```

Calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve) on a logistic regression using the full dataset. Don't forget the random effects!

```{r performance measures fun}
data_ups_NAs$PredictionsPerc=predict(m_range,type="response") #needed to return probabilities 0-1


#this should be turned to a function to make the cross validation function nicer
performance = function (data) {
  data$Predictions[data$PredictionsPerc>0.5]='Schizophrenia'
  data$Predictions[data$PredictionsPerc<=0.5]='Control'
  c_matrix=confusionMatrix(data = data$Predictions, reference = data$Diagnosis,positive='Control')
  
  print(c_matrix)
  #extract the measurements
  Accuracy=c_matrix$overall[[1]]
  Sensitivity=c_matrix$byClass[[1]]
  Specificity=c_matrix$byClass[[2]]
  PPV=c_matrix$byClass[[3]]
  NPV=c_matrix$byClass[[4]]
  
  #ROC
  rocCurve <- roc(response = data$Diagnosis, predictor =data$PredictionsPerc)
  a=auc(rocCurve)#from 0 to 1 (+ being good)
  AUC=a[1]
  ROC_plot=plot(rocCurve, legacy.axes = TRUE)
  
  
  output = cbind(Accuracy,Sensitivity,Specificity,PPV,NPV,AUC)
  output=as.data.frame(output)
  print(c_matrix)
  print(ROC_plot)
  return(output)
}

performance (data_ups_NAs)

```

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures.

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?

```{r cross validation fun}

crossvalidate = function(model,nfold,data) {
  predictions = data.frame()
  `%not in%` <- function (x, table) is.na(match(x, table, nomatch=NA_integer_))
  Folds = createFolds(unique(data$Subject),nfold)
  for (f in Folds) {
    train_d = subset(data, Subject %not in% f)
    #subset including only 1 fold
    test_d = subset(data, Subject %in% f)
    #fit train subset into specified model
    model_val = update(model, data = train_d)
    test_d$PredictionsPerc=predict(model_val,test_d,type="response",allow.new.levels=TRUE)
    prediction_fold = cbind(test_d$Subject,test_d$Diagnosis,test_d$PredictionsPerc)
    predictions = rbind(predictions,prediction_fold)
  }
  
  predictions = plyr::rename(predictions, c("V1"="Subject","V2"="Diagnosis","V3"="PredictionsPerc"))
  predictions$Diagnosis = as.factor(predictions$Diagnosis)
  predictions$Diagnosis = plyr::revalue(predictions$Diagnosis, c('1'="Control", "2"="Schizophrenia"))
  results=performance(predictions)
  
  return(results)
  return(predictions)
}

crossvalidate(m_range,6,data_ups_NAs) #works
```

### Question 2 - Which single acoustic predictor is the best predictor of diagnosis?
```{r single predictor}
model_mean = glmer(Diagnosis ~ mean + (1+Trial | Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_sd = glmer(Diagnosis ~ sd + (1+Trial| Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_range = glmer(Diagnosis ~ range + (1 +Trial| Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_median = glmer(Diagnosis ~ median + (1 +Trial| Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_iqr = glmer(Diagnosis ~ iqr + (1 +Trial| Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_absmeandev = glmer(Diagnosis ~ abs_mean_dev + (1 +Trial| Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_cofvar = glmer(Diagnosis ~ cof_var + (1 +Trial| Subject),data_ups_NAs,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_RR = glmer(Diagnosis ~ RR  + (1 +Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_DET= glmer(Diagnosis ~ DET  + (1 +Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_maxL = glmer(Diagnosis ~ maxL  + (1 +Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_L = glmer(Diagnosis ~ L  + (1 +Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_ENTR = glmer(Diagnosis ~ ENTR+ (1 +Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_LAM = glmer(Diagnosis ~ LAM + (1 +Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))

model_TT = glmer(Diagnosis ~ TT + (1 +Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))
```

```{r summaries}
summary(model_mean) #BIC 1880.2 z=5,961 ***
summary(model_sd) #
summary(model_range) # **
summary(model_median) #**
summary(model_iqr)#
summary(model_absmeandev) #
summary(model_cofvar) #***
summary(model_RR) #*
summary(model_DET) #***
summary(model_maxL) #
summary(model_L) #
summary(model_ENTR) #***
summary(model_LAM) #***
summary(model_TT) # *** 

#put models in a table based on their BIC (penalized for n observations)
model_list = list(model_mean,model_range,model_median,model_cofvar,model_RR,model_DET,model_ENTR,model_LAM,model_TT)
AICcmodavg::bictab(model_list,sort=T,modnames=c("mean","range","median","cofvar","RR","DET", "ENTR","LAM", "TT"))
```

```{r cross validate}
#cross validate 10x times to get more robust results

model_list_stand = list(model_mean,model_range,model_median,model_cofvar)

model_list_rqa = list(model_RR,model_DET,model_ENTR,model_LAM,model_TT)

CV_results = data.frame()
n=0
while (n < 10) {
  for (model in model_list_stand) {
  result_1 = crossvalidate(model, nfold = 6,data_ups_NAs)
  name = as.character(model@call$formula[3])
  result_1 = cbind(result_1, name)
  CV_results = rbind(CV_results, result_1)
  }
  n = n + 1
}

n=0
while (n < 10) {
  for (model in model_list_rqa) {
  result_1 = crossvalidate(model, nfold = 6,data_ups)
  name = as.character(model@call$formula[3])
  result_1 = cbind(result_1, name)
  CV_results = rbind(CV_results, result_1)
  }
  n = n + 1
}

```

```{r CV single pred result}
#let's find out which single predictor results in the best results
CV_single_pred = CV_results %>%
  dplyr::group_by(name) %>%
  dplyr::summarise(
  accuracy = round(mean(Accuracy),3),
  accuracy_int = paste0(round(min(Accuracy),3),"-",round(max(Accuracy),3)),
  sensitivity = round(mean(Sensitivity),3),
  sensitivity_int = paste0(round(min(Sensitivity),3),"-",round(max(Sensitivity),3)),
  specificity = round(mean(Specificity),3),
  specificity_int = paste0(round(min(Specificity),3),"-",round(max(Specificity),3)),
  ppv = round(mean(PPV),3),
  ppv_int = paste0(round(min(PPV),3),"-",round(max(PPV),3)),
  npv = round(mean(NPV),3),
  npv_int = paste0(round(min(NPV),3),"-",round(max(NPV),3)),
  auc = round(mean(AUC),3),
  acu_int = paste0(round(min(AUC),3),"-",round(max(AUC),3))
  )

#coefficient of variation is the best single predictor, second is trapping time based on AUC, NPV and PPV
```
### Question 3 - Which combination of acoustic predictors is best for diagnosing schizophrenia?

Now it's time to go wild! Use all (voice-related) variables and interactions you can think of. Compare models and select the best performing model you can find.

Remember:
- Out-of-sample error crucial to build the best model!
- After choosing the model, send Celine and Riccardo the code of your model

```{r go wild}
#first all significant predictors
model_1=glmer(Diagnosis ~ mean+median+cof_var+range+LAM+RR+DET+ENTR+TT+ (1+Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))
summary(model_1) #BIC=1344
crossvalidate(model_1,6,data_ups)

#this seems to be the best and simplest model
model_2 = glmer(Diagnosis ~ scale(mean)+scale(cof_var)+scale(RR)+ (1+Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))
summary(model_2) #1326,6
crossvalidate(model_2,6,data_ups)

#now try add interactions
model_3 = glmer(Diagnosis ~ scale(mean)+scale(cof_var)+scale(RR)+scale(mean)*scale(cof_var)+ (1+Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))
summary(model_3) #1317.2
crossvalidate(model_3,6,data_ups) #auc=.6486

model_4 = glmer(Diagnosis ~ scale(mean)+scale(cof_var)+scale(RR)+scale(mean)*scale(RR)+scale(mean)*scale(cof_var)+ (1+Trial| Subject),data_ups,family=binomial,control = glmerControl(calc.derivs = FALSE))
summary(model_4) #1315
crossvalidate(model_4,6,data_ups) #auc=.6428
```

```{r}
model_l = list(model_1,model_2,model_3,model_4)
CV_results_2 = data.frame()
n=0
while (n < 10) {
  for (model in model_l) {
    result_1 = crossvalidate(model,nfold = 4,data = data_ups)
    name = as.character(model@call$formula[3])
    result_1 = cbind(result_1, name)
    CV_results_2 = rbind(CV_results_2, result_1)
  }
  n=n+1
}

CV_res_2=CV_results_2 %>% 
  dplyr::group_by(name) %>%
  dplyr::summarise(
  accuracy = round(mean(Accuracy),3),
  accuracy_int = paste0(round(min(Accuracy),3),"-",round(max(Accuracy),3)),
  sensitivity = round(mean(Sensitivity),3),
  sensitivity_int = paste0(round(min(Sensitivity),3),"-",round(max(Sensitivity),3)),
  specificity = round(mean(Specificity),3),
  specificity_int = paste0(round(min(Specificity),3),"-",round(max(Specificity),3)),
  ppv = round(mean(PPV),3),
  ppv_int = paste0(round(min(PPV),3),"-",round(max(PPV),3)),
  npv = round(mean(NPV),3),
  npv_int = paste0(round(min(NPV),3),"-",round(max(NPV),3)),
  auc = round(mean(AUC),3),
  acu_int = paste0(round(min(AUC),3),"-",round(max(AUC),3))
  )
```

### Question 4: Properly report the results

METHODS SECTION: how did you analyse the data? That is, how did you extract the data, designed the models and compared their performance?

RESULTS SECTION: can you diagnose schizophrenia based on voice? which features are used? Comment on the difference between the different performance measures.

### Bonus question 5

You have some additional bonus data involving speech rate, pauses, etc. Include them in your analysis. Do they improve classification?

### Bonus question 6

Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them.

```{r random forest}
library(randomForest)
#remove unnecesary variables (min, max etc)
forest_data = data_ups[-8]
write.csv(forest_data,"forest_data.csv",row.names = F)

ctrl = trainControl(method="repeatedcv", number=6,repeats = 12, selectionFunction = "oneSE", savePredictions = "final", classProbs = T)
in_train = createDataPartition(forest_data$Subject, p=.8, list=FALSE)
train_forest = train(Diagnosis ~ .-Subject,data=forest_data, method='rf', metric="Kappa",trControl=ctrl, subset = in_train)
train_forest
rf=train_forest$finalModel

#look at predictions saved from cross validation
confusionMatrix(train_forest$pred$pred,train_forest$pred$obs)

#test the model
test = forest_data[-in_train,]
test$PredictionsPerc=predict(train_forest,test,type="prob",allow.new.levels=T,predict.all=T)
test$PredictionsPerc = test$PredictionsPerc$Schizophrenia
performance(test)

varImp(train_forest)
```

```{r black box no more}
library(inTrees)

#modify data for inTrees solution
#dataframe with only Diagnosis
target = forest_data[in_train,2]

#and dataframe with predictors only
predictors = forest_data[in_train,-1:-2]

#unbox random forest
treeList = RF2List(rf) # transform rf object to an inTrees' format
exec = extractRules(treeList,predictors,ntree=200) # R-executable conditions
exec = unique(exec)
ruleMetric = getRuleMetric(exec,predictors,target) # get rule metrics
ruleMetric = pruneRule(ruleMetric,predictors,target)
ruleMetric = selectRuleRRF(ruleMetric,predictors,target)
learner = buildLearner(ruleMetric,predictors,target)
Simp_Learner = presentRules(ruleMetric,colnames(predictors))

write.csv2(Simp_Learner,'forest_rules.csv',row.names = F)
```


```{r neural network}
#just for fun
library(elmNN)
ctrl = trainControl(method="repeatedcv", number=10,repeats = 10, selectionFunction = "oneSE")
in_train = createDataPartition(data_ups$Subject, p=.7, list=FALSE)
train_neural = train(Diagnosis ~mean+median+range+cof_var+RR+DET+ENTR+LAM+TT,data=data_ups, method="nnet", metric="Kappa",trControl=ctrl, subset = in_train)
train_neural

test = data_ups[-in_train,]
test$PredictionsPerc=predict(train_neural,test,type="raw",allow.new.levels=T,predict.all=T)
confusionMatrix(test$PredictionsPerc,test$Diagnosis)
```